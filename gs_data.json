{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "9VhMC1QAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Xuyang Liu", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=9VhMC1QAAAAJ&citpid=21", "affiliation": "Sichuan University", "organization": 5231920307941946883, "interests": ["Vision-language Models", "Efficient AI", "Transfer Learning"], "email_domain": "@stu.scu.edu.cn", "homepage": "https://xuyang-liu16.github.io/", "citedby": 420, "publications": {"9VhMC1QAAAAJ:TQgYirikUcIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accelerating Diffusion Transformers with Token-wise Feature Caching", "pub_year": 2025, "author": "Chang Zou* and Xuyang Liu* and Ting Liu and Siteng Huang and Linfeng Zhang", "journal": "International Conference on Learning Representations (ICLR)", "abstract": "Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10 more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36 and 1.93 acceleration are achieved on OpenSora and PixArt- with almost no drop in generation quality."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:TQgYirikUcIC", "num_citations": 64, "citedby_url": "/scholar?cites=9696062613102340406", "cites_id": ["9696062613102340406"], "pub_url": "https://arxiv.org/abs/2410.05317", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:NinpHZNVj4YJ:scholar.google.com/", "cites_per_year": {"2024": 2, "2025": 54, "2026": 8}}, "9VhMC1QAAAAJ:blknAaTinKkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark", "pub_year": 2024, "author": "Yi Xin* and Siqi Luo* and Xuyang Liu* and Yuntao Du* and Haodi Zhou and Xinyu Cheng and Christina Luoluo Lee and Junlong Du and Haozhe Wang and MingCai Chen and Ting Liu and Guimin Hu and Zhongwei Wan and Rongchao Zhang and Aoxue Li and Mingyang Yi and Xiaohong Liu", "journal": "Advances in Neural Information Processing Systems", "volume": "37", "pages": "80522-80535", "abstract": "Parameter-efficient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient. To address this challenge, we construct a Unified Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting 30 diverse, challenging, and comprehensive datasets from image recognition, video action recognition, and dense prediction tasks. On these datasets, we systematically evaluate 25 dominant PETL algorithms and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately 310 GPU days. We release all the benchmark, making it more efficient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:blknAaTinKkC", "num_citations": 54, "citedby_url": "/scholar?cites=17743828768370716039", "cites_id": ["17743828768370716039"], "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/935de67d1a033fd517cb49d192b5c008-Abstract-Datasets_and_Benchmarks_Track.html", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:h-0XBkG-PvYJ:scholar.google.com/", "cites_per_year": {"2024": 4, "2025": 45, "2026": 4}}, "9VhMC1QAAAAJ:mB3voiENLucC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models", "pub_year": 2026, "author": "Xuyang Liu and Ziming Wang and Yuhang Han and Yingyao Wang and Jiale Yuan and Jun Song and Bo Zheng and Linfeng Zhang and Siteng Huang and Honggang Chen", "journal": "AAAI Conference on Artificial Intelligence (AAAI)", "abstract": "Large vision-language models (LVLMs) excel at visual understanding, but face efficiency challenges due to quadratic complexity in processing long multi-modal contexts. While token compression can reduce computational costs, existing approaches are designed for single-view LVLMs and fail to consider the unique multi-view characteristics of high-resolution LVLMs with dynamic cropping. Existing methods treat all tokens uniformly, but our analysis reveals that global thumbnails can naturally guide the compression of local crops by providing holistic context for informativeness evaluation. In this paper, we first analyze dynamic cropping strategy, revealing both the complementary nature between thumbnails and crops, and the distinctive characteristics across different crops. Based on our observations, we propose ``Global Compression Commander'' (\\textit{i.e.}, \\textbf{GlobalCom}), a novel plug-and-play token compression framework for HR-LVLMs. GlobalCom leverages thumbnail as the ``commander'' to guide the compression of local crops, adaptively preserving informative details while eliminating redundancy. Extensive experiments show that GlobalCom maintains over \\textbf{90\\%} performance while compressing \\textbf{90\\%} visual tokens, reducing FLOPs and peak memory to \\textbf{9.1\\%} and \\textbf{60\\%}."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:mB3voiENLucC", "num_citations": 34, "citedby_url": "/scholar?cites=6145777281003963452,12847056739468876881,13647893300263217368,9930450546401590857", "cites_id": ["6145777281003963452", "12847056739468876881", "13647893300263217368", "9930450546401590857"], "pub_url": "https://arxiv.org/abs/2501.05179", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Sb7ZfSgM0IkJ:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 29, "2026": 4}}, "9VhMC1QAAAAJ:bEWYMUwI8FkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Filter, Correlate, Compress: Training-Free Token Reduction for MLLM Acceleration", "pub_year": 2026, "author": "Yuhang Han* and Xuyang Liu* and Pengxiang Ding and Donglin Wang and Honggang Chen and Qingsen Yan and Siteng Huang", "journal": "AAAI Conference on Artificial Intelligence (AAAI)", "abstract": "The quadratic complexity of Multimodal Large Language Models (MLLMs) with respect to context length poses significant computational and memory challenges, hindering their real-world deployment. In the paper, we devise a ''filter-correlate-compress'' framework to accelerate the MLLM by systematically optimizing multimodal context length during prefilling. The framework first implements FiCoCo-V, a training-free method operating within the vision encoder. It employs a redundancy-based token discard mechanism that uses a novel integrated metric to accurately filter out redundant visual tokens. To mitigate information loss, the framework introduces a correlation-based information recycling mechanism that allows preserved tokens to selectively recycle information from correlated discarded tokens with a self-preserving compression, thereby preventing the dilution of their own core content. The framework's FiCoCo-L variant further leverages task-aware textual priors to perform token reduction directly within the LLM decoder. Extensive experiments demonstrate that the FiCoCo series effectively accelerates a range of MLLMs, achieves up to 14.7x FLOPs reduction with 93.6% performance retention. Our methods consistently outperform state-of-the-art training-free approaches, showcasing effectiveness and generalizability across model architectures, sizes, and tasks without requiring retraining. Code: https://github.com/kawhiiiileo/FiCoCo"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:bEWYMUwI8FkC", "num_citations": 33, "citedby_url": "/scholar?cites=8794276652755652230,11400262616627503782", "cites_id": ["8794276652755652230", "11400262616627503782"], "pub_url": "https://arxiv.org/abs/2411.17686", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:pmI_2nLeNZ4J:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 30, "2026": 2}}, "9VhMC1QAAAAJ:hMod-77fHWUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Shifting AI Efficiency from Model-Centric to Data-Centric Compression", "pub_year": 2025, "author": "Xuyang Liu and Zichen Wen and Shaobo Wang and Junjie Chen and Zhishan Tao and Yubo Wang and Xiangqi Jin and Chang Zou and Yiyu Wang and Chenfei Liao and Xu Zheng and Honggang Chen and Weijia Li and Xuming Hu and Conghui He and Linfeng Zhang", "journal": "arXiv preprint arXiv:2505.19147", "abstract": "The advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on scaling model parameters. However, as hardware limits constrain further model growth, the primary computational bottleneck has shifted to the quadratic cost of self-attention over increasingly long sequences by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \\textbf{we argue that the focus of research for efficient artificial intelligence (AI) is shifting from model-centric compression to data-centric compression}. We position data-centric compression as the emerging paradigm, which improves AI efficiency by directly compressing the volume of data processed during model training or inference. To formalize this shift, we establish a unified framework for existing efficiency strategies and demonstrate why it constitutes a crucial paradigm change for long-context AI. We then systematically review the landscape of data-centric compression methods, analyzing their benefits across diverse scenarios. Finally, we outline key challenges and promising future research directions. Our work aims to provide a novel perspective on AI efficiency, synthesize existing efforts, and catalyze innovation to address the challenges posed by ever-increasing context lengths."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:hMod-77fHWUC", "num_citations": 31, "citedby_url": "/scholar?cites=4793831317793941708,17462354863943388747,7061907751766439322", "cites_id": ["4793831317793941708", "17462354863943388747", "7061907751766439322"], "pub_url": "https://arxiv.org/abs/2505.19147", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mrWdRK_vAGIJ:scholar.google.com/", "cites_per_year": {"2024": 2, "2025": 28, "2026": 1}}, "9VhMC1QAAAAJ:ns9cj8rnVeAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "GUI-G: Gaussian Reward Modeling for GUI Grounding", "pub_year": 2026, "author": "Fei Tang and Zhangxuan Gu and Zhengxi Lu and Xuyang Liu and Shuheng Shen and Changhua Meng and Wen Wang and Wenqi Zhang and Yongliang Shen and Weiming Lu and Jun Xiao and Yueting Zhuang", "journal": "AAAI Conference on Artificial Intelligence (AAAI)", "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:ns9cj8rnVeAC", "num_citations": 28, "citedby_url": "/scholar?cites=7060620921301341982,14177003197367897240", "cites_id": ["7060620921301341982", "14177003197367897240"], "pub_url": "https://arxiv.org/abs/2507.15846", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mMywhkvVvsQJ:scholar.google.com/", "cites_per_year": {"2025": 23, "2026": 5}}, "9VhMC1QAAAAJ:J_g5lzvAfSwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models", "pub_year": 2025, "author": "Xuyang Liu and Yiyu Wang and Junpeng Ma and Linfeng Zhang", "journal": "Empirical Methods in Natural Language Processing (EMNLP)", "abstract": "Video large language models (VideoLLM) excel at video understanding, but face efficiency challenges due to the quadratic complexity of abundant visual tokens. Our systematic analysis of token compression methods for VideoLLMs reveals two critical issues:(i) overlooking distinctive visual signals across frames, leading to information loss;(ii) suffering from implementation constraints, causing incompatibility with modern architectures or efficient operators. To address these challenges, we distill three design principles for VideoLLM token compression and propose a plug-and-play inference acceleration framework “Video Compression Commander”(VidCom 2). By quantifying each frame’s uniqueness, VidCom 2 adaptively adjusts compression intensity across frames, effectively preserving essential information while reducing redundancy in video sequences. Extensive experiments across various VideoLLMs and benchmarks demonstrate the superior performance and efficiency of our VidCom 2. With only 25% visual tokens, VidCom 2 achieves 99.6% of the original performance on LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame Compression Adjustment strategy is compatible with other token compression methods to further improve their performance. Our code is available at https://github. com/xuyang-liu16/VidCom2."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:J_g5lzvAfSwC", "num_citations": 28, "citedby_url": "/scholar?cites=16607032690607409801,11884502444319363390", "cites_id": ["16607032690607409801", "11884502444319363390"], "pub_url": "https://aclanthology.org/2025.emnlp-main.98/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:PtHqnQE87qQJ:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 25, "2026": 2}}, "9VhMC1QAAAAJ:9ZlFYXVOiuMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders", "pub_year": 2024, "author": "Xuyang Liu and Siteng Huang and Yachen Kang and Honggang Chen and Donglin Wang", "conference": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "pages": "2765-2769", "publisher": "IEEE", "abstract": "Large-scale text-to-image diffusion models have shown impressive capabilities for generative tasks by leveraging strong vision-language alignment from pre-training. However, most vision-language discriminative tasks require extensive fine-tuning on carefully-labeled datasets to acquire such alignment, with great cost in time and computing resources. In this work, we explore directly applying a pre-trained generative diffusion model to the challenging discriminative task of visual grounding without any fine-tuning and additional training dataset. Specifically, we propose VGDiffZero, a simple yet effective zero-shot visual grounding framework based on text-to-image diffusion models. We also design a comprehensive region-scoring method considering both global and local contexts of each isolated proposal. Extensive experiments on RefCOCO, RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:9ZlFYXVOiuMC", "num_citations": 21, "citedby_url": "/scholar?cites=8687101331233228541", "cites_id": ["8687101331233228541"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/10445945/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_cb6b6LIjngJ:scholar.google.com/", "cites_per_year": {"2024": 11, "2025": 10}}, "9VhMC1QAAAAJ:isC4tDSrTZIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "M2IST: Multi-Modal Interactive Side-Tuning for Efficient Referring Expression Comprehension", "pub_year": 2025, "author": "Xuyang Liu and Ting Liu and Siteng Huang and Yi Xin and Yue Hu and Long Qin and Donglin Wang and Yuanyuan Wu and Honggang Chen", "journal": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)", "publisher": "IEEE", "abstract": "Referring expression comprehension (REC) is a vision-language task to locate a target object in an image based on a language expression. Fully fine-tuning general-purpose pre-trained vision-language foundation models for REC yields impressive performance but becomes increasingly costly. Parameter-efficient transfer learning (PETL) methods have shown strong performance with fewer tunable parameters. However, directly applying PETL to REC faces two challenges: (1) insufficient multi-modal interaction between pre-trained vision-language foundation models, and (2) high GPU memory usage due to gradients passing through the heavy vision-language foundation models. To this end, we present M2IST: Multi-Modal Interactive Side-Tuning with M3ISAs: Mixture of Multi-Modal Interactive Side-Adapters. During fine-tuning, we fix the pre-trained uni-modal encoders and update M3ISAs to enable efficient …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:isC4tDSrTZIC", "num_citations": 19, "citedby_url": "/scholar?cites=3301366871514887654", "cites_id": ["3301366871514887654"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/10929057/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:5jXjoa3P0C0J:scholar.google.com/", "cites_per_year": {"2024": 4, "2025": 14, "2026": 1}}, "9VhMC1QAAAAJ:ZeXyd9-uunAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Sparse-Tuning: Adapting vision transformers with efficient fine-tuning and inference", "pub_year": 2024, "author": "Ting Liu* and Xuyang Liu* and Siteng Huang and Liangtao Shi and Zunnan Xu and Yi Xin and Quanjun Yin and Xiaohong Liu", "journal": "arXiv preprint arXiv:2405.14700", "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for adapting pre-trained Vision Transformer (ViT) models to downstream applications by updating only a small subset of parameters. While current PEFT methods have achieved fine-tuning efficiency, they overlook the efficiency of computation and GPU memory during inference, falling short of practical requirements. To address this limitation, we propose Sparse-Tuning, an efficient and effective framework that leverages popular token sparsification (TS) techniques to reduce information redundancy in images and videos, thereby significantly improving computational and memory efficiency. However, TS often compromises performance due to inevitable information loss. To address this limitation, we further introduce Dense Adapters (DA) to compensate for the information losses incurred by token sparsification. DA integrates comprehensive token information from shallow layers into the retained tokens of deeper layers, ensuring minimal performance degradation. Through the integration of TS techniques and DA, Sparse-Tuning achieves a significant reduction in computation and memory overhead while maintaining performance. Empirical results on VTAB-1K, three image datasets, and two video datasets show that Sparse-Tuning reduces GFLOPs to 66\\% of the original ViT-B while achieving state-of-the-art performance compared to full fine-tuning and other PEFT baselines."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:ZeXyd9-uunAC", "num_citations": 19, "citedby_url": "/scholar?cites=2242206580160713443", "cites_id": ["2242206580160713443"], "pub_url": "https://arxiv.org/abs/2405.14700", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:49KFQwHrHR8J:scholar.google.com/", "cites_per_year": {"2024": 3, "2025": 16}}, "9VhMC1QAAAAJ:4TOpqqG69KYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DARA: Domain-and Relation-aware Adapters Make Parameter-efficient Tuning for Visual Grounding", "pub_year": 2024, "author": "Ting Liu* and Xuyang Liu* and Siteng Huang and Honggang Chen and Quanjun Yin and Long Qin and Donglin Wang and Yue Hu", "journal": "IEEE International Conference on Multimedia and Expo (ICME)", "abstract": "Visual grounding (VG) is a challenging task to localize an object in an image based on a textual description. Recent surge in the scale of VG models has substantially improved performance, but also introduced a significant burden on computational costs during fine-tuning. In this paper, we explore applying parameter-efficient transfer learning (PETL) to efficiently transfer the pre-trained vision-language knowledge to VG. Specifically, we propose DARA, a novel PETL method comprising Domain-aware Adapters (DA Adapters) and Relation-aware Adapters (RA Adapters) for VG. DA Adapters first transfer intra-modality representations to be more fine-grained for the VG domain. Then RA Adapters share weights to bridge the relation between two modalities, improving spatial reasoning. Empirical results on widely-used benchmarks demonstrate that DARA achieves the best accuracy while saving numerous updated …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:4TOpqqG69KYC", "num_citations": 17, "citedby_url": "/scholar?cites=14132004256576852880", "cites_id": ["14132004256576852880"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/10688132/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:kFOSLf_2HsQJ:scholar.google.com/", "cites_per_year": {"2024": 5, "2025": 12}}, "9VhMC1QAAAAJ:HDshCWvjkbEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Perception-and Fidelity-aware Reduced-reference Super-resolution Image Quality Assessment", "pub_year": 2024, "author": "Xinying Lin and Xuyang Liu and Hong Yang and Xiaohai He and Honggang Chen", "journal": "IEEE Transactions on Broadcasting (TBC)", "publisher": "IEEE", "abstract": "With the advent of image super-resolution (SR) algorithms, how to evaluate the quality of generated SR images has become an urgent task. Although full-reference methods perform well in SR image quality assessment (SR-IQA), their reliance on high-resolution (HR) images limits their practical applicability. Leveraging available reconstruction information as much as possible for SR-IQA, such as low-resolution (LR) images and the scale factors, is a promising way to enhance assessment performance for SR-IQA without HR for reference. In this paper, we attempt to evaluate the perceptual quality and reconstruction fidelity of SR images considering LR images and scale factors. Specifically, we propose a novel dual-branch reduced-reference SR-IQA network, i.e., Perception- and Fidelity-aware SR-IQA (PFIQA). The perception-aware branch evaluates the perceptual quality of SR images by leveraging the merits of …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:HDshCWvjkbEC", "num_citations": 16, "citedby_url": "/scholar?cites=430675805973907351", "cites_id": ["430675805973907351"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/10742110/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:lwduHF0R-gUJ:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 14, "2026": 1}}, "9VhMC1QAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "GLMLP-TRANS: A transportation mode detection model using lightweight sensors integrated in smartphones", "pub_year": 2022, "author": "Xuyang Liu", "journal": "Computer Communications", "volume": "194", "pages": "156-166", "publisher": "Elsevier", "abstract": "Transportation mode detection (TMD), as an essential part of Intelligent Transportation Systems, aims at analyzing human current transportation activities, and can be widely adopted in road planning and traffic prediction. Although several relevant works have been conducted on TMD, there are still some great challenges due to complex factors including: (i) availability that several sensors are limited in certain scenarios; (ii) lightweight that both data collection sensors and the architecture of most current neural network are heavy and sophisticated; and (iii) expert knowledge that numerous researches for TMD are based on traditional machine learning. To address these challenges, we propose a deep learning-based transportation mode detection network using smartphone-based sensors called GLMLP-TRANS, which is inspired by Self-attention and MLP-Mixer. Our proposed GLMLP-TRANS network can capture …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:qjMakFHDy7sC", "num_citations": 14, "citedby_url": "/scholar?cites=7130048953887918872", "cites_id": ["7130048953887918872"], "pub_url": "https://www.sciencedirect.com/science/article/pii/S0140366422002535", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:GIuiN78F82IJ:scholar.google.com/", "cites_per_year": {"2022": 1, "2023": 2, "2024": 2, "2025": 7, "2026": 2}}, "9VhMC1QAAAAJ:BqipwSGYUEgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "pub_year": 2026, "author": "Zichen Wen and Jiashu Qu and Dongrui Liu and Zhiyuan Liu and Ruixi Wu and Yicun Yang and Xiangqi Jin and Haoyun Xu and Xuyang Liu and Weijia Li and Chaochao Lu and Jing Shao and Conghui He and Linfeng Zhang", "journal": "International Conference on Learning Representations (ICLR)", "abstract": "Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:BqipwSGYUEgC", "num_citations": 10, "citedby_url": "/scholar?cites=13739064108721232684", "cites_id": ["13739064108721232684"], "pub_url": "https://arxiv.org/abs/2507.11097", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:LNulogP2qr4J:scholar.google.com/", "cites_per_year": {"2025": 8, "2026": 2}}, "9VhMC1QAAAAJ:RYcK_YlVTxYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Variation-aware Vision Token Dropping for Faster Large Vision-Language Models", "pub_year": 2025, "author": "Junjie Chen* and Xuyang Liu* and Zichen Wen and Yiyu Wang and Siteng Huang and Honggang Chen", "journal": "arXiv preprint arXiv:2509.01552", "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding tasks. However, the increasing demand for high-resolution image and long-video understanding results in substantial token counts, leading to reduced inference efficiency. Token compression offers a direct solution by reducing the number of tokens to be processed, thereby improving computational efficiency. Through extensive analysis, we identify two critical limitations in existing inner-LLM token compression methods: positional bias and incompatibility with efficient operators, which hinder their practical deployment for LVLM acceleration. This paper presents the first approach from a token variation perspective, revealing that visual token variations within LLMs exhibit task-agnostic properties. We propose Variation-aware Vision Token Dropping (\\textit{i.e.}, \\textbf{VDrop}), which progressively removes visual tokens with minimal variation during LVLM inference, thereby enhancing computational efficiency. Extensive experiments across multiple models and benchmarks demonstrate that our VDrop is able to maintain \\textbf{94.0\\%} and \\textbf{98.6\\%} of the original model performance for image and video understanding tasks respectively, while reducing LLM generation latency by \\textbf{31.5\\%} and \\textbf{74.2\\%}. When combined with efficient operators, VDrop further reduces GPU peak memory usage."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:RYcK_YlVTxYC", "num_citations": 8, "citedby_url": "/scholar?cites=17060474890180189664", "cites_id": ["17060474890180189664"], "pub_url": "https://arxiv.org/abs/2509.01552", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:4F2puIX7wuwJ:scholar.google.com/", "cites_per_year": {"2025": 8}}, "9VhMC1QAAAAJ:mVmsd5A6BfQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers", "pub_year": 2025, "author": "Shaobo Wang and Hongxuan Tang and Mingyang Wang and Hongrui Zhang and Xuyang Liu and Weiya Li and Xuming Hu and Linfeng Zhang", "journal": "International Conference on Learning Representations (ICLR)", "abstract": "The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, AutoGnothi, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. AutoGnothi enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that AutoGnothi offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:mVmsd5A6BfQC", "num_citations": 7, "citedby_url": "/scholar?cites=7264282812509421529", "cites_id": ["7264282812509421529"], "pub_url": "https://arxiv.org/abs/2410.21815", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:2X-Z67rqz2QJ:scholar.google.com/", "cites_per_year": {"2024": 2, "2025": 5}}, "9VhMC1QAAAAJ:NaGl4SEjCO4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving", "pub_year": 2025, "author": "Minhao Xiong and Zichen Wen and Zhuangcheng Gu and Xuyang Liu and Rui Zhang and Hengrui Kang and Jiabing Yang and Junyuan Zhang and Weijia Li and Conghui He and Yafei Wang and Linfeng Zhang", "journal": "arXiv preprint arXiv:2508.13305", "abstract": "Vision-Language Models (VLMs) have emerged as a promising paradigm in autonomous driving (AD), offering a unified framework for perception, reasoning, and decision-making by jointly modeling visual inputs and natural language instructions. However, their deployment is hindered by the significant computational overhead incurred when processing high-resolution, multi-view images, a standard setup in AD systems with six or more synchronized cameras. This overhead stems from the large number of visual tokens generated during encoding, increasing inference latency and memory consumption due to the quadratic complexity of self-attention. To address these challenges, we propose Prune2Drive, a plug-and-play visual token pruning framework for multi-view VLMs in autonomous driving. Prune2Drive introduces two core innovations: (i) a diversity-aware token selection mechanism inspired by farthest point sampling, which prioritizes semantic and spatial coverage across views rather than relying solely on attention scores, and (ii) a view-adaptive pruning controller that learns optimal pruning ratios for each camera view based on their importance to downstream driving tasks. Unlike prior methods, Prune2Drive does not require model retraining or access to attention maps, making it compatible with modern efficient attention implementations. Extensive experiments on two large-scale multi-view driving benchmarks, DriveLM and DriveLMM-o1, show that Prune2Drive achieves significant speedups and memory savings while maintaining or improving task performance. When retaining only 10% of the visual tokens, our method achieves a 6.40 …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:NaGl4SEjCO4C", "num_citations": 6, "citedby_url": "/scholar?cites=5250660983428230298", "cites_id": ["5250660983428230298"], "pub_url": "https://arxiv.org/abs/2508.13305", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:miw-YVgY3kgJ:scholar.google.com/", "cites_per_year": {"2025": 5, "2026": 1}}, "9VhMC1QAAAAJ:35N4QoGY0k4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Ai for service: Proactive assistance with ai glasses", "pub_year": 2025, "author": "Zichen Wen and Yiyu Wang and Chenfei Liao and Boxue Yang and Junxian Li and Weifeng Liu and Haocong He and Bolong Feng and Xuyang Liu and Yuanhuiyi Lyu and Xu Zheng and Xuming Hu and Linfeng Zhang", "journal": "arXiv preprint arXiv:2510.14359", "abstract": "In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:35N4QoGY0k4C", "num_citations": 4, "citedby_url": "/scholar?cites=16796837212098845906", "cites_id": ["16796837212098845906"], "pub_url": "https://arxiv.org/abs/2510.14359", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:0tw2cXNaGukJ:scholar.google.com/", "cites_per_year": {"2025": 3, "2026": 1}}, "9VhMC1QAAAAJ:70eg2SAEIzsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models", "pub_year": 2026, "author": "Xuyang Liu and Xiyan Gui and Yuchao Zhang and Linfeng Zhang", "journal": "International Conference on Learning Representations (ICLR)", "abstract": "Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \\texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \\texttt{MixKV} improves baseline methods by an average of \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:70eg2SAEIzsC", "num_citations": 3, "citedby_url": "/scholar?cites=10924538301876937275", "cites_id": ["10924538301876937275"], "pub_url": "https://arxiv.org/abs/2510.20707", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:OyKGcrTBm5cJ:scholar.google.com/", "cites_per_year": {"2024": 1, "2025": 1, "2026": 1}}, "9VhMC1QAAAAJ:pqnbT2bcN3wC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders", "pub_year": 2025, "author": "Yuan Chen and Zichen Wen and Yuzhou Wu and Xuyang Liu and Shuang Chen and Junpeng Ma and Weijia Li and Conghui He and Linfeng Zhang", "journal": "arXiv preprint arXiv:2512.18747", "abstract": "Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:pqnbT2bcN3wC", "num_citations": 2, "citedby_url": "/scholar?cites=13914443327714623108", "cites_id": ["13914443327714623108"], "pub_url": "https://arxiv.org/abs/2512.18747", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hP67E3wIGsEJ:scholar.google.com/", "cites_per_year": {"2025": 1, "2026": 1}}, "9VhMC1QAAAAJ:M05iB0D1s5AC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression", "pub_year": 2025, "author": "Yiyu Wang* and Xuyang Liu* and Xiyan Gui and Xinying Lin and Boxue Yang and Chenfei Liao and Tailai Chen and Linfeng Zhang", "journal": "arXiv preprint arXiv:2512.00891", "abstract": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:M05iB0D1s5AC", "num_citations": 1, "citedby_url": "/scholar?cites=11564692819524394991", "cites_id": ["11564692819524394991"], "pub_url": "https://arxiv.org/abs/2512.00891", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:7x9D1NgKfqAJ:scholar.google.com/", "cites_per_year": {"2025": 1}}, "9VhMC1QAAAAJ:TFP_iSt0sucC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models", "pub_year": 2025, "author": "Junjie Chen and Xuyang Liu and Subin Huang and Linfeng Zhang and Hang Yu", "journal": "IEEE Transactions on Computational Social Systems (TCSS)", "abstract": "With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous “neutral” cases. We further validate our findings on a diverse 100-sample mini-benchmark, incorporating multiple datasets, expanded prompt variants, and representative commercial LVLMs. Our findings reveal notable discrepancies—across LVLMs and within the same model under varied prompts …"}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:TFP_iSt0sucC", "num_citations": 1, "citedby_url": "/scholar?cites=6589465135382985100", "cites_id": ["6589465135382985100"], "pub_url": "https://ieeexplore.ieee.org/abstract/document/11206397/", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jPlSzqB7clsJ:scholar.google.com/", "cites_per_year": {"2025": 1}}, "9VhMC1QAAAAJ:rO6llkc54NcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models", "pub_year": 2026, "author": "Yue Ding and Yiyan Ji and Jungang Li and Xuyang Liu and Xinlong Chen and Junfei Wu and Bozhou Li and Bohan Zeng and Yang Shi and Yushuo Guan and Yuanxing Zhang and Jiaheng Liu and Qiang Liu and Pengfei Wan and Liang Wang", "journal": "arXiv preprint arXiv:2602.04804", "abstract": "Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks."}, "filled": true, "author_pub_id": "9VhMC1QAAAAJ:rO6llkc54NcC", "num_citations": 0, "pub_url": "https://arxiv.org/abs/2602.04804", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:BoGw4-6S-XIJ:scholar.google.com/", "cites_per_year": {}}}, "citedby5y": 419, "hindex": 13, "hindex5y": 13, "i10index": 14, "i10index5y": 14, "cites_per_year": {"2023": 2, "2024": 40, "2025": 340, "2026": 36}, "updated": "2026-02-19T05:36:08.572933+00:00"}